{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: implementando LVQ em Python\n",
    "\n",
    "Um dos principais pontos fracos (ou desvantagens) do kNN é que ele te obriga a depender do seu conjunto de dados **inteiro**. Isso porque para fornecer uma classificação para uma instância desconhecida, ele a compara com **todas** as instâncias conhecidas.\n",
    "\n",
    "O  algoritmo *Learning Vector Quantization* (LVQ) é como se fosse uma solução para esse problema. Ele é parecido com o kNN, porém com ele você pode escolher o tamanho do conjunto de dados que você será \"obrigado\" a depender.\n",
    "\n",
    "Ele ainda realiza sua classificação a partir de uma comparação com instâncias conhecidas, assim como o  kNN. A diferença é que essa comparação não precisa ser feita com todo o conjunto de dados. Apenas aquelas instâncias que melhor resumem o conjunto são levadas em consideração!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pegando a intuição\n",
    "\n",
    "Se tivermos uma quantidade muito gigantesca de dados, rodar um programa que explora o conjunto inteiro pode ser muito caro, como no caso do kNN.\n",
    "\n",
    "O ideal seria diminuir o tamanho desse conjunto de dados, ou fazer um resumo dele. E isso pode ser feito pegando as instâncias que melhor resumem, ou melhor representam, o restante do conjunto.\n",
    "\n",
    "Considere a imagem abaixo. Nela, podemos ver uma distribuição de várias instâncias de um conjunto de dados qualquer, com três classes: vermelho, verde e azul. Se fôssemos aplicar o kNN com um valor de k igual a 1 (por exemplo), é assim que o modelo faria suas classificações:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![i](src/lvq.png)\n",
    "*Fonte: wikipedia*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado um ponto novo e desconhecido, bastaria posicioná-lo na imagem acima e ver qual a cor da região em que ele se encaixa. Essa seria a sua classificação.\n",
    "\n",
    "Agora, perceba que em cada região dessas, existem vários pontos da mesma classe. Por exemplo, na região azul mais central, tem vários pontos azuis, assim como a região vermelha de cima tem vários pontos vermelhos.\n",
    "\n",
    "A sacada do LVQ é pegar somente um ou alguns pontos que fiquem bem no centro dessas regiões, como se fizesse um resumo dessa quantidade tão grande de pontos. Na imagem abaixo, esses pontos representantes são os círculos: \n",
    "\n",
    "![img](src/lvqresumindo.gif)\n",
    "*Fonte: [Willamette University](https://www.willamette.edu/~gorr/classes/cs449/Unsupervised/competitive.html)*\n",
    "\n",
    "\n",
    "O resultado é um espaço subdividido em regiões conhecido como **Diagrama de Voronoi**, conforme a figura abaixo:\n",
    "\n",
    "![img](src/voronoi.gif)\n",
    "*Fonte: [Willamette University](https://www.willamette.edu/~gorr/classes/cs449/Unsupervised/competitive.html)*\n",
    "\n",
    "\n",
    "Feito isso, basta usar esses pontos que são representativos do todo, e aplicar o kNN considerando-os como o novo e resumido conjunto de dados!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando\n",
    "\n",
    "A representação para o LVQ é uma coleção específica de instâncias, que serão **parecidas** com as instâncias presentes no seu conjunto de dados. Aos elementos dessa coleção, daremos o nome de **vetores**.Esses vetores são aprendidos ou descobertos a partir do conjunto de dados de treino. \n",
    "\n",
    "Nessa implementação, iremos aprender a partir de um conjunto de dados produzido com o sinal de 16 antenas de alta frequência (radares). O alvo dessas antenas são os elétrons livres na ionosfera (parte superior da atmosfera, acima da estratosfera). Sinais \"bons\" de radar são aqueles que exibem evidência da presença de algum tipo de estrutura na ionosfera. Sinais \"ruins\" não. Seus sinais passam direto através da ionosfera. [Mais detalhes](https://archive.ics.uci.edu/ml/datasets/Ionosphere)\n",
    "\n",
    "Resumindo: esses dados que usaremos trazem informações a respeito da estrutura da ionosfera de acordo com o retorno dos radares. Cada instância descreve as propriedades do retorno do radar, e nosso objetivo será predizer se tem ou não tem algum tipo de estrutura na ionosfera.\n",
    "\n",
    "São 351 registros no total. Cada um é composto por 34 valores numéricos, e mais um valor que pode ser \"g\" para sinal bom (good), e \"b\" para sinal ruim (bad)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medida de similaridade: distância euclidiana\n",
    "\n",
    "Para descobrir o quanto um dado vetor é bom em representar os demais, precisaremos de uma medida de similaridade: o quanto esse vetor está próximo dos outros. Para isso, usamos a distância euclidiana, que pode ser usada para espaços de qualquer dimensão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def distancia_euclidiana(linha1, linha2):\n",
    "\tdistancia = 0.0\n",
    "\tfor i in range(len(linha1)-1):\n",
    "\t\tdistancia += (linha1[i] - linha2[i])**2\n",
    "\treturn sqrt(distancia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que essa função assume que a última coluna de cada linha é a classificação (\"g\" ou \"b\"), que deve ser ignorada para o cálculo da distância\n",
    "\n",
    "Agora, podemos usar esse cálculo da distância para localizar o vetor que mais se aproxima de um novo dado. Para fazer isso, calculamos a distância entre cada vetor e o novo dado (usando a função acima).\n",
    "\n",
    "Uma vez que as distâncias forem calculadas, podemos fazer uma ordenação para pegar aquele vetor que tiver a menor distância. Abaixo, o código implementa isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pegar_vetor_mais_proximo(vetores, instancia_teste):\n",
    "\tdistancias = list()\n",
    "\tfor vetor in vetores:\n",
    "\t\tdist = distancia_euclidiana(vetor, instancia_teste)\n",
    "\t\tdistancias.append((vetor, dist))\n",
    "\tdistancias.sort(key=lambda tup: tup[1])\n",
    "\treturn distancias[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos testar essa função com um pequeno conjunto de dados inventado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.7810836, 2.550537003, 0]\n"
     ]
    }
   ],
   "source": [
    "conjunto_dados = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "instancia_teste = conjunto_dados[0]\n",
    "vetor = pegar_vetor_mais_proximo(conjunto_dados, instancia_teste)\n",
    "print(vetor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rodando esse exemplo, vemos o vetor mais próximo ao primeiro registro do conjunto. Como esperado, o primeiro registro é o mais próximo dele mesmo!\n",
    "\n",
    "Agora que já sabemos como encontrar o vetor mais próximo, temos que aprender como faz para encontrar quais são os vetores que melhor resumem o restante do conjunto.\n",
    "\n",
    "### Treinando os vetores\n",
    "\n",
    "O primeiro passo para o treinamento é inicializar o conjunto de vetores. Uma maneira de fazer isso é pegar valores aleatórios que estão presentes no nosso conjunto de dados, e ir montando os vetores assim.\n",
    "\n",
    "O código abaixo recebe o conjunto de dados de treino, e pega colunas e linhas aleatórias para montar um vetor completamente aleatório (mas com valores encontrado nos dados reais):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vetor_aleatorio(dados):\n",
    "\tqtd_registros = len(dados)\n",
    "\tqtd_colunas = len(dados[0])\n",
    "\tvetor = [dados[randrange(qtd_registros)][i] for i in range(qtd_colunas)]\n",
    "\treturn vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que inicializamos os vetores aleatórios, devemos ajustá-los para que eles sejam uma coisa que faz mais sentido, de um jeito que sejam uma boa representação do restante do conjunto. E isso será feito iterativamente:\n",
    "\n",
    "1. Epochs ou Épocas: De maneira geral, o processo vai ser repetido um número fixo de vezes. Em cada época, os vetores são expostos ao conjunto de treino e adaptados a ele.\n",
    "2. Conjunto de treino: Dentro de uma época, cada instância do nosso conjunto de dados é usado, um de cada vez, para ajustar o valor dos nossos vetores.\n",
    "\n",
    "Esse ajuste é feito da seguinte forma:\n",
    "\n",
    "1. Se o vetor é da mesma classe da instância da vez, então empurramos o vetor para mais perto da instância\n",
    "2. Se o vetor **não** é da mesma classe da instância da vez, então empurramos o vetor para mais **longe** da instância\n",
    "\n",
    "Para cada instância dentro do conjunto de dados, nós estudamos apenas o vetor mais próximo. E ele, e somente ele, vai ser ajustado com base nessa instância.\n",
    "\n",
    "Além disso, nós podemos usar a diferença entre esse vetor e essa instância do conjunto de dados como uma métrica para saber qual o tamanho do erro do nosso algoritmo. É esse erro que usamos para decidir quanto vamos empurrar o nosso vetor, para mais perto ou para mais longe de um elemento do conjunto.\n",
    "\n",
    "Ainda por cima, vamos ajustar essa quantidade por uma taxa de aprendizado (learning rate). Por exemplo, uma taxa de aprendizado de 0.3 quer dizer que os vetores apenas são empurrados por 30% do erro ou diferença entre eles e a instância.\n",
    "\n",
    "E essa taxa de aprendizado será ajustada também. Queremos que ela tenha o máximo efeito na primeira época, e vá decaindo conforme o treinamento continue, até que seu efeito seja minimizado nas últimas épocas. Fazemos isso porque, como os vetores são inicializados aleatoriamente, provavelmente eles estão muito longe do estado ótimo ou ideal. Mas conforme vamos treinando, eles estão ficando cada vez melhores, então não queremos que eles mudem rápido demais, pra evitar de deixar passar o estado ideal.\n",
    "\n",
    "Podemos expressar essa taxa de aprendizado assim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxa = taxa_aprendizado * (1.0 - (epoca/epocas_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E podemos testar essa equação colocando uma taxa de aprendizado de 0.3 e 10 épocas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Epoca\t\tTaxa de aprendizado\n",
    "0\t\t0.3\n",
    "1\t\t0.27\n",
    "2\t\t0.24\n",
    "3\t\t0.21\n",
    "4\t\t0.18\n",
    "5\t\t0.15\n",
    "6\t\t0.12\n",
    "7\t\t0.09\n",
    "8\t\t0.06\n",
    "9\t\t0.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, podemos juntar tudo isso. Abaixo, a função implementa o procedimento para treinar um conjunto de vetores, dado um conjunto de dados para treino.\n",
    "\n",
    "Só que essa função recebe três outras informações importantes: quantos vetores criar e treinar, a taxa de aprendizado e por quantas épocas deve durar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def treinar_vetores(dados, qtd_vetores, taxa_aprendizado, epocas):\n",
    "\tvetores = [vetor_aleatorio(dados) for i in range(qtd_vetores)]\n",
    "\tfor epoca in range(epocas):\n",
    "\t\ttaxa = taxa_aprendizado * (1.0-(epoca/float(epocas)))\n",
    "\t\tsoma_erros = 0.0\n",
    "\t\tfor linha in dados:\n",
    "\t\t\tvetor = pegar_vetor_mais_proximo(vetores, linha)\n",
    "\t\t\tfor i in range(len(linha)-1):\n",
    "\t\t\t\terro = linha[i] - vetor[i]\n",
    "\t\t\t\tsoma_erros += erro**2\n",
    "\t\t\t\tif vetor[-1] == linha[-1]:\n",
    "\t\t\t\t\tvetor[i] += taxa * erro\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tvetor[i] -= taxa * erro\n",
    "\t\tprint('>epoca=%d, taxa_aprendizado=%.3f, erro=%.3f' % (epoca, taxa, soma_erros))\n",
    "\treturn vetores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que essa função também mostra os erros em cada época, junto com a taxa de aprendizado. E ela faz uso das outras funções que definimos mais acima, para inicializar os vetores e encontrar o vetor mais próximo.\n",
    "\n",
    "Vamos demonstrar o uso de tudo o que fizemos até aqui com um conjunto de dados inventado:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoca=0, taxa_aprendizado=0.300, erro=139.812\n",
      ">epoca=1, taxa_aprendizado=0.270, erro=47.368\n",
      ">epoca=2, taxa_aprendizado=0.240, erro=27.535\n",
      ">epoca=3, taxa_aprendizado=0.210, erro=26.241\n",
      ">epoca=4, taxa_aprendizado=0.180, erro=25.509\n",
      ">epoca=5, taxa_aprendizado=0.150, erro=24.778\n",
      ">epoca=6, taxa_aprendizado=0.120, erro=24.053\n",
      ">epoca=7, taxa_aprendizado=0.090, erro=23.344\n",
      ">epoca=8, taxa_aprendizado=0.060, erro=22.653\n",
      ">epoca=9, taxa_aprendizado=0.030, erro=21.981\n",
      "Vetores: [[7.3188612290158614, 1.9696349335193466, 1], [2.4304257696446854, 2.8396012380964555, 0]]\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from random import randrange\n",
    "from random import seed\n",
    "\n",
    "# calcular a distancia euclidiana entre dois vetores\n",
    "def distancia_euclidiana(linha1, linha2):\n",
    "\tdistancia = 0.0\n",
    "\tfor i in range(len(linha1)-1):\n",
    "\t\tdistancia += (linha1[i] - linha2[i])**2\n",
    "\treturn sqrt(distancia)\n",
    "\n",
    "# localizar o vetor mais próximo\n",
    "def pegar_vetor_mais_proximo(vetores, instancia_teste):\n",
    "\tdistancias = list()\n",
    "\tfor vetor in vetores:\n",
    "\t\tdist = distancia_euclidiana(vetor, instancia_teste)\n",
    "\t\tdistancias.append((vetor, dist))\n",
    "\tdistancias.sort(key=lambda tup: tup[1])\n",
    "\treturn distancias[0][0]\n",
    "\n",
    "# criar um vetor aleatório\n",
    "def vetor_aleatorio(dados):\n",
    "\tqtd_registros = len(dados)\n",
    "\tqtd_colunas = len(dados[0])\n",
    "\tvetor = [dados[randrange(qtd_registros)][i] for i in range(qtd_colunas)]\n",
    "\treturn vetor\n",
    "\n",
    "# treinar um conjunto de vetores\n",
    "def treinar_vetores(dados, qtd_vetores, taxa_aprendizado, epocas):\n",
    "\tvetores = [vetor_aleatorio(dados) for i in range(qtd_vetores)]\n",
    "\tfor epoca in range(epocas):\n",
    "\t\ttaxa = taxa_aprendizado * (1.0-(epoca/float(epocas)))\n",
    "\t\tsoma_erros = 0.0\n",
    "\t\tfor linha in dados:\n",
    "\t\t\tvetor = pegar_vetor_mais_proximo(vetores, linha)\n",
    "\t\t\tfor i in range(len(linha)-1):\n",
    "\t\t\t\terro = linha[i] - vetor[i]\n",
    "\t\t\t\tsoma_erros += erro**2\n",
    "\t\t\t\tif vetor[-1] == linha[-1]:\n",
    "\t\t\t\t\tvetor[i] += taxa * erro\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tvetor[i] -= taxa * erro\n",
    "\t\tprint('>epoca=%d, taxa_aprendizado=%.3f, erro=%.3f' % (epoca, taxa, soma_erros))\n",
    "\treturn vetores\n",
    "\n",
    "# testar a função que treina\n",
    "seed(1)\n",
    "conjunto_dados = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "taxa_aprendizado = 0.3\n",
    "qtd_epocas = 10\n",
    "qtd_vetores = 2\n",
    "vetores = treinar_vetores(conjunto_dados, qtd_vetores, taxa_aprendizado, qtd_epocas)\n",
    "print('Vetores: %s' % vetores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse exemplo treina um conjunto de 2 vetores ao longo de 10 épocoas com uma taxa de aprendizado de 0.3. Os detalhes são mostrados com o print a cada época e os vetores no final são exibidos.\n",
    "\n",
    "Note como o erro começa grande no começo, e vai gradualmente caindo.\n",
    "\n",
    "\n",
    "Finalmente, agora que sabemos como treinar esses vetores para resumir um conjunto de dados, vamos aplicar esse algoritmo em um conjunto de dados real!\n",
    "\n",
    "### Caso de estudo: Ionosfera\n",
    "\n",
    "O primeiro passo é carregar o conjunto de dados e converter suas informações para valores numéricos que podemos usar nos cálculos de distância. Para isso, fazemos funções para:\n",
    "1. carregar os dados de um arquivo csv\n",
    "2. converter números em string para float\n",
    "3. converter a coluna de classe para valores inteiros\n",
    "\n",
    "Em seguida, também vamos fazer funções que irão validar o nosso algoritmo usando [validação cruzada](https://pt.wikipedia.org/wiki/Valida%C3%A7%C3%A3o_cruzada)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O exemplo completo tá aqui:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acertos: [90.0, 88.57142857142857, 84.28571428571429, 87.14285714285714, 85.71428571428571]\n",
      "Media de acertos: 87.143%\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    "\n",
    "# ler arquivo CSV\n",
    "def carregar_csv(nome_arquivo):\n",
    "\tconjunto_dados = list()\n",
    "\twith open(nome_arquivo, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor linha in csv_reader:\n",
    "\t\t\tif not linha:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tconjunto_dados.append(linha)\n",
    "\treturn conjunto_dados\n",
    "\n",
    "# converter coluna de strings para coluna de floats\n",
    "def str_coluna_para_float(conjunto_dados, coluna):\n",
    "\tfor linha in conjunto_dados:\n",
    "\t\tlinha[coluna] = float(linha[coluna].strip())\n",
    "\n",
    "# converter coluna de strings para coluna de inteiros\n",
    "def str_coluna_para_int(conjunto_dados, coluna):\n",
    "\tclasses = [linha[coluna] for linha in conjunto_dados]\n",
    "\tunicas = set(classes)\n",
    "\tresultado = dict()\n",
    "\tfor i, valor in enumerate(unicas):\n",
    "\t\tresultado[valor] = i\n",
    "\tfor linha in conjunto_dados:\n",
    "\t\tlinha[coluna] = resultado[linha[coluna]]\n",
    "\treturn resultado\n",
    "\n",
    "# particionar o conjunto de dados em qtd_partes\n",
    "def cross_validation(conjunto_dados, qtd_partes):\n",
    "\tparte_conjunto = list()\n",
    "\tcopia_conjunto = list(conjunto_dados)\n",
    "\ttamanho_parte = int(len(conjunto_dados) / qtd_partes)\n",
    "\tfor i in range(qtd_partes):\n",
    "\t\tparte = list()\n",
    "\t\twhile len(parte) < tamanho_parte:\n",
    "\t\t\tindice = randrange(len(copia_conjunto))\n",
    "\t\t\tparte.append(copia_conjunto.pop(indice))\n",
    "\t\tparte_conjunto.append(parte)\n",
    "\treturn parte_conjunto\n",
    "\n",
    "# Calcular a eficacia em percentual\n",
    "def medida_eficacia(real, predicao):\n",
    "\tcorretos = 0\n",
    "\tfor i in range(len(real)):\n",
    "\t\tif real[i] == predicao[i]:\n",
    "\t\t\tcorretos += 1\n",
    "\treturn corretos / float(len(real)) * 100.0\n",
    "\n",
    "# Avaliar um algoritmo usando o cross-validation\n",
    "def avaliar_algoritmo(conjunto_dados, algoritmo, qtd_partes, *args):\n",
    "\tpartes = cross_validation(conjunto_dados, qtd_partes)\n",
    "\tacertos = list()\n",
    "\tfor parte in partes:\n",
    "\t\tconjunto_treino = list(partes)\n",
    "\t\tconjunto_treino.remove(parte)\n",
    "\t\tconjunto_treino = sum(conjunto_treino, [])\n",
    "\t\tconjunto_teste = list()\n",
    "\t\tfor linha in parte:\n",
    "\t\t\tcopia_linha = list(linha)\n",
    "\t\t\tconjunto_teste.append(copia_linha)\n",
    "\t\t\tcopia_linha[-1] = None\n",
    "\t\tpredicao = algoritmo(conjunto_treino, conjunto_teste, *args)\n",
    "\t\treal = [linha[-1] for linha in parte]\n",
    "\t\teficacia = medida_eficacia(real, predicao)\n",
    "\t\tacertos.append(eficacia)\n",
    "\treturn acertos\n",
    "\n",
    "# calcular a distancia euclidiana entre dois vetores\n",
    "def distancia_euclidiana(linha1, linha2):\n",
    "\tdistancia = 0.0\n",
    "\tfor i in range(len(linha1)-1):\n",
    "\t\tdistancia += (linha1[i] - linha2[i])**2\n",
    "\treturn sqrt(distancia)\n",
    "\n",
    "# localizar o vetor mais próximo\n",
    "def pegar_vetor_mais_proximo(vetores, instancia_teste):\n",
    "\tdistancias = list()\n",
    "\tfor vetor in vetores:\n",
    "\t\tdist = distancia_euclidiana(vetor, instancia_teste)\n",
    "\t\tdistancias.append((vetor, dist))\n",
    "\tdistancias.sort(key=lambda tup: tup[1])\n",
    "\treturn distancias[0][0]\n",
    "\n",
    "# fazer uma predição dentre os vetores mais próximos\n",
    "def predizer(vetores, instancia_teste):\n",
    "\tvetor = pegar_vetor_mais_proximo(vetores, instancia_teste)\n",
    "\treturn vetor[-1]\n",
    "\n",
    "# criar um vetor aleatório\n",
    "def vetor_aleatorio(dados):\n",
    "\tqtd_registros = len(dados)\n",
    "\tqtd_colunas = len(dados[0])\n",
    "\tvetor = [dados[randrange(qtd_registros)][i] for i in range(qtd_colunas)]\n",
    "\treturn vetor\n",
    "\n",
    "# treinar um conjunto de vetores\n",
    "def treinar_vetores(dados, qtd_vetores, taxa_aprendizado, epocas):\n",
    "\tvetores = [vetor_aleatorio(dados) for i in range(qtd_vetores)]\n",
    "\tfor epoca in range(epocas):\n",
    "\t\ttaxa = taxa_aprendizado * (1.0-(epoca/float(epocas)))\n",
    "\t\tsoma_erros = 0.0\n",
    "\t\tfor linha in dados:\n",
    "\t\t\tvetor = pegar_vetor_mais_proximo(vetores, linha)\n",
    "\t\t\tfor i in range(len(linha)-1):\n",
    "\t\t\t\terro = linha[i] - vetor[i]\n",
    "\t\t\t\tsoma_erros += erro**2\n",
    "\t\t\t\tif vetor[-1] == linha[-1]:\n",
    "\t\t\t\t\tvetor[i] += taxa * erro\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tvetor[i] -= taxa * erro\n",
    "\treturn vetores\n",
    "\n",
    "# nosso LVQ numa função\n",
    "def learning_vector_quantization(dados, teste, qtd_vetores, taxa_aprendizado, epocas):\n",
    "\tvetores = treinar_vetores(dados, qtd_vetores, taxa_aprendizado, epocas)\n",
    "\tpredicoes = list()\n",
    "\tfor linha in teste:\n",
    "\t\toutput = predizer(vetores, linha)\n",
    "\t\tpredicoes.append(output)\n",
    "\treturn(predicoes)\n",
    "\n",
    "# Testar o LVQ nos dados da ionosfera\n",
    "seed(1)\n",
    "# carregar e preparar os dados\n",
    "nome_arquivo = 'src/ionosfera.csv'\n",
    "conjunto_dados = carregar_csv(nome_arquivo)\n",
    "for i in range(len(conjunto_dados[0])-1):\n",
    "\tstr_coluna_para_float(conjunto_dados, i)\n",
    "# converter a coluna de classes para inteiros\n",
    "str_coluna_para_int(conjunto_dados, len(conjunto_dados[0])-1)\n",
    "# avaliar o algoritmo\n",
    "qtd_partes = 5\n",
    "taxa_aprendizado = 0.3\n",
    "qtd_epocas = 50\n",
    "qtd_vetores = 20\n",
    "acertos = avaliar_algoritmo(conjunto_dados, learning_vector_quantization, qtd_partes, qtd_vetores, taxa_aprendizado, qtd_epocas)\n",
    "print('Acertos: %s' % acertos)\n",
    "print('Media de acertos: %.3f%%' % (sum(acertos)/float(len(acertos))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sobre o LVQ\n",
    "\n",
    "O LVQ é mais usado como um algoritmo de classificação. Ele suporta problemas de classificação tanto binários (com apenas duas classes), quanto multi-classes.\n",
    "\n",
    "Contudo, como esse algoritmo trabalha fazendo resumo de um corpo maior de informações, ele também pode ser utilizado para compressão de dados. Por exemplo, se quisermos enviar informações através de uma linha telefônica nós devemos:\n",
    "1. inicialmente enviar os vetores\n",
    "2. para cada informação a ser transmitida, nós mandamos somente o índice do vetor que é mais similar (ao invés da informação em si)\n",
    "\n",
    "Para uma grande quantidade de dados, isso pode ser uma redução significante. Por exemplo, se treinarmos um conjunto de 64 vetores, então só são necessários 6 bits para representar (indexar) esses vetores. E, se os nossos dados forem números em notação de ponto flutuante (floats, que \"pesam\" 4 bytes), então teríamos uma redução de 80%  ( 100*(1 - 6/32) ).\n",
    "\n",
    "E isso pode ser aplicado em diferentes tipos de dados: números (floats), imagens, sinal sonoro..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
